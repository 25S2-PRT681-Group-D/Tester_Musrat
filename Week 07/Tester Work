1. Test cases to Agro Scan’s requirements & user stories:
Functional Test Cases
Test Case ID	Scenario	Precondition	Test Steps	Expected Result	Actual Result
TC-FR01-001	Register with valid email & password	User not registered	Enter valid email + strong password → Submit	Account created, redirected to login	
TC-FR01-002	Register with duplicate email	Email already exists	Enter duplicate email → Submit	Error “Email already in use”	
TC-FR01-003	Register with invalid password	—	Enter “abc123” → Submit	Error “Password must be ≥8 chars, 1 number & 1 symbol”	
TC-FR01-004	Login with valid credentials	Registered user	Enter correct email + password → Login	JWT token returned & stored	
TC-FR01-005	Login with invalid credentials	Registered user	Enter wrong password → Login	Error “Invalid credentials”	
TC-FR01-006	Logout clears session	User logged in	Click Logout	User redirected to login, JWT cleared	
TC-FR02-001	Update profile name	User logged in	Change name → Save	Updated immediately	
TC-FR02-002	Update profile with invalid email	User logged in	Change email to “abc@com” → Save	Error “Invalid email format”	
TC-FR02-003	Update profile with duplicate email	Another user has same email	Change email → Save	Error “Email already exists”	
TC-FR03-001	Create inspection record with valid photo & plant name	User logged in	Upload JPG ≤5MB + enter plant name → Save	Record saved with timestamp	
TC-FR03-002	Upload invalid file type	—	Upload PDF → Save	Error “Invalid file type”	
TC-FR03-003	Create inspection with future date	—	Enter date 2050 → Save	Error “Date cannot be in future”	
TC-FR04-001	AI analysis within 15s	Inspection exists	Upload valid photo → Run AI Analysis	Diagnosis + confidence % + treatment suggestions shown ≤15s	
TC-FR04-002	AI timeout handling	Simulate >15s response	Run AI Analysis	Timeout message + retry option shown	
TC-FR05-001	View inspection history (latest first)	User has ≥3 records	Open History page	Records sorted by latest date	
TC-FR05-002	Paginated history (20 per page)	User has ≥25 records	Open History page	20 per page, navigation works	
TC-FR06-001	Edit inspection (owner only)	User owns record	Edit notes → Save	Changes saved & updated immediately	
TC-FR06-002	Edit inspection (non-owner)	Record belongs to another user	Attempt edit	System shows 403 Unauthorized	
TC-FR07-001	Delete inspection with confirmation	Record exists	Select record → Delete → Confirm	Record removed from list (soft delete in DB)	
TC-FR07-002	Cancel delete confirmation	Record exists	Select record → Delete → Cancel	Record still visible	
TC-FR08-001	Unauthorized access to inspection	Not logged in	API request without token	Response: 403 Forbidden	

Non Functional Test cases
Test Case ID	Scenario	Precondition	Test Steps	Expected Result	Actual Result
TC-NFR01	Verify AI response time ≤15s	Inspection exists	Upload plant photo → Trigger AI Analysis	Results returned within 15 seconds	
TC-NFR02	Verify page load ≤2s	—	Load dashboard page	Page fully loads in ≤2 seconds	
TC-NFR03	Verify TLS 1.3 encryption	—	Inspect network traffic	All requests use HTTPS TLS 1.3	
TC-NFR04	Verify access control (data isolation)	Multiple users	User A tries to access User B’s record	Access denied, response 403	
TC-NFR05	Verify password storage with bcrypt + salt	DB configured	Inspect DB/code after registration	Password stored hashed, not plain text	
TC-NFR06	Verify mobile-first responsive design	—	Open app on phone, tablet, desktop	Layout adjusts correctly	
TC-NFR07	Verify error message clarity	—	Enter invalid email → Submit	Error shown: “Invalid email format”	
TC-NFR08	Verify DB scalability (10,000+ users)	DB seeded	Run load test with 10K+ users	DB handles load without crash	
TC-NFR09	Verify AI backend concurrency (100 requests)	—	Simulate 100 AI requests in parallel	All requests processed, no failure	
TC-NFR10	Verify uptime ≥99.9%	Monitoring enabled	Observe uptime for 30 days	SLA ≥99.9% met	
TC-NFR11	Verify backup retention (30 days)	Backup configured	Run daily backup job → Check logs	Backups retained for 30 days	
TC-NFR12	Verify cross-browser compatibility	—	Test app on Chrome, Firefox, Safari, Edge (latest 2 versions)	App works consistently across browsers	


3. Knowledge Document: Software Testing Options & Methodologies
1. Introduction
Testing is a critical part of the Software Development Life Cycle (SDLC). It ensures that applications are reliable, secure, user-friendly, and meet business requirements. Choosing the right testing methodologies depends on project scope, risk, technology, and delivery timelines.
2. Categories of Testing
2.1 Functional Testing
Validates what the system does against the specified requirements.
•	Unit Testing – Tests individual components or modules (e.g., a function or API endpoint).
•	Integration Testing – Ensures modules work correctly together.
•	System Testing – Validates the entire system’s functionality.
•	User Acceptance Testing (UAT) – Final validation by business stakeholders/end-users.
Tools: JUnit, NUnit, Selenium, Postman.

2.2 Non-Functional Testing
Validates how the system performs.
•	Performance Testing – Measures response time, scalability, and reliability.
o	Subtypes: Load Testing, Stress Testing, Endurance Testing.
•	Security Testing – Identifies vulnerabilities, data leaks, or weak authentication.
•	Usability Testing – Ensures intuitive user experience (UX).
•	Compatibility Testing – Checks performance across devices, browsers, OS.
Tools: JMeter, LoadRunner, OWASP ZAP, BrowserStack.

2.3 Automated vs. Manual Testing
•	Manual Testing – Human testers execute test cases; useful for exploratory, usability, and ad-hoc testing.
•	Automated Testing – Uses scripts/tools for regression, performance, and repetitive tasks.
Trade-off: Manual is flexible but slow; Automation is fast but requires setup/maintenance.
3. Testing Methodologies
3.1 Agile Testing
•	Integrated with Agile development (Scrum/Kanban).
•	Continuous feedback, test-driven by user stories.
•	Test types: Acceptance Test-Driven Development (ATDD), Behavior-Driven Development (BDD).
•	Benefit: Early bug detection, faster releases.
3.2 Waterfall Testing
•	Sequential phase-based (requirements → design → build → test → deploy).
•	Testing occurs late in the cycle.
•	Benefit: Structured, good for stable requirements.
•	Risk: Bugs found late, high cost of rework.
3.3 V-Model (Verification & Validation)
•	Testing phase mirrors development phase.
•	Each stage has associated testing (e.g., Requirements → Acceptance Testing).
•	Benefit: Structured mapping of test activities.
3.4 Risk-Based Testing
•	Focuses on high-risk areas (e.g., financial transactions, authentication).
•	Benefit: Prioritizes critical features under tight deadlines.
3.5 Shift-Left Testing
•	Testing is introduced earlier in the SDLC.
•	Encourages Test-Driven Development (TDD) and Continuous Integration (CI).
•	Benefit: Catches bugs early, reduces costs.
4. Best Practices
•	Define clear Test Plans & Test Cases.
•	Maintain traceability between requirements and test cases.
•	Adopt CI/CD pipelines with automated regression testing.
•	Include exploratory testing to find edge cases.
•	Regularly update test data and test environments.
5. Summary Table
Methodology	Focus Area	When to Use	Example Tools
Agile Testing	Continuous testing	Iterative, fast delivery	Cucumber, JUnit
Waterfall	Sequential phase	Fixed, stable projects	Manual, HP ALM
V-Model	Verification & Validation	High compliance projects	Selenium, QTP
Risk-Based	Critical areas	Limited resources	Risk Matrix
Shift-Left	Early testing in SDLC	CI/CD pipelines	GitHub Actions, Jenkins

6. Conclusion
Selecting the right mix of testing methodologies ensures software quality, reduces production risks, and improves customer satisfaction. Teams should tailor the testing approach based on project needs, timelines, and business priorities.


